{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba15b3-1379-4a96-9dbd-f4d2ac8af028",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is a Support Vector Machine (SVM)\n",
    " Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It tries to find the best boundary known as hyperplane that separates different classes in the data. It is useful when you want to do binary classification like spam vs. not spam or cat vs. dog.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f46febc-735b-4fbe-9848-0056af8ee32e",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the difference between Hard Margin and Soft Margin SVM\n",
    "Support Vector Machines (SVMs) are powerful tools for classification and regression tasks. The concept of margin is central to SVMs, representing the distance between the decision boundary (hyperplane) and the closest data points from each class. There are two main approaches to margins in SVMs: hard margin and soft margin.\n",
    "\n",
    "Hard Margin SVM\n",
    "\n",
    "In a hard margin SVM, the goal is to find a hyperplane that completely separates the data points of different classes with the maximum possible margin. This approach is suitable for linearly separable data where no misclassifications are allowed. The decision boundary is determined by the support vectors, which are the data points closest to the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d79a1b-3475-4333-a69b-a4b5ea3c2209",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the mathematical intuition behind SVM\n",
    "The mathematical intuition behind Support Vector Machines (SVM) involves the following concepts: \n",
    "Towards Data Science\n",
    "+1\n",
    "Hyperplane: SVM uses a hyperplane to separate two classes in a dataset.\n",
    "Margin: The goal is to find a hyperplane with the maximal distance (margin) to the support vectors of both classes.\n",
    "Maximizing Margin: SVM finds the boundary that maximizes the margin between the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1cae5-96b4-4519-aad9-0d633ed66d55",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are Support Vectors in SVM\n",
    "Support Vector Machines (SVMs) are a powerful supervised learning algorithm used for classification, regression, and outlier detection. The main objective of SVM is to find the optimal hyperplane that separates the data points of different classes in the feature space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c7ec7-bda5-477f-8554-421a68819dab",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is a Support Vector Classifier (SVC)\n",
    "Support Vector Classification (SVC) is a powerful supervised machine learning algorithm used for classification tasks. It is based on the Support Vector Machine (SVM) algorithm, which can handle both linear and non-linear classification problems. SVC aims to find the optimal hyperplane that separates data points of different classes with the maximum margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0e356-0a36-45b2-8ab0-a0bf1d07851b",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is a Support Vector Regressor (SVR)\n",
    "Support Vector Regression (SVR) is a type of Support Vector Machine (SVM) used for regression tasks. Unlike traditional regression models, SVR employs the principles of SVM to predict continuous output values. It aims to find a function that best approximates the relationship between input features and continuous target variables by transforming the input features into high-dimensional spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2d1d5-1d73-4ee5-a8c6-d5ae8670bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the Kernel Trick in SVM\n",
    "classify non-linear data\n",
    "The Kernel Trick is a method used in Support Vector Machines (SVM) to classify non-linear data by projecting it onto a higher dimension space where it can be linearly divided by a plane. This is achieved by using Lagrangian formula with Lagrangian multipliers. Instead of computing data coordinates using a mapping function and training/testing model, the kernel trick is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97efd2-158f-451e-92ef-f961829321ea",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the effect of the C parameter in SVM\n",
    "he C parameter in Support Vector Machines (SVM) is a crucial hyperparameter that controls the trade-off between achieving a low training error and allowing for misclassifications. It plays a significant role in shaping the decision boundary and influencing the complexity and accuracy of the model\n",
    "1\n",
    "2\n",
    ".\n",
    "\n",
    "Understanding the C Parameter\n",
    "\n",
    "The C parameter is a regularization parameter that balances two competing goals:\n",
    "\n",
    "Maximizing the margin: The distance between the hyperplane and the nearest data points.\n",
    "\n",
    "Minimizing the number of misclassifications: Ensuring that the training data is classified correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa992f6-32f1-4a64-8eb6-8179cd6baceb",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the role of the Gamma parameter in RBF Kernel SVM\n",
    "Gamma parameter: The gamma parameter in SVMs is a hyperparameter that controls the shape of the decision boundary. It determines the flexibility of the model and the level of overfitting or underfitting of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4878bd0-78d6-4ac4-bc8f-923c7cace771",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the Naïve Bayes classifier, and why is it called \"Naïve\"\n",
    "The Naive Bayes algorithm is a classification algorithm based on Bayes' theorem. The algorithm assumes that the features are independent of each other, which is why it is called \"naive.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250ccef-573e-4e97-874f-8c01e75ed5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is Bayes’ Theorem\n",
    "ayes' Theorem is a fundamental concept in probability theory and statistics, named after the Reverend Thomas Bayes. It provides a way to update the probability of a hypothesis based on new evidence. The theorem is used to determine the conditional probability of an event, given the occurrence of another event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d2257-7e20-4218-96cf-11e358472637",
   "metadata": {},
   "outputs": [],
   "source": [
    " Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
    "Gaussian Naive Bayes\n",
    "Gaussian Naive Bayes is useful when working with continuous values which probabilities can be modeled using a Gaussian distribution:\n",
    "\n",
    "Multinomial naive Bayes\n",
    "A multinomial distribution is useful to model feature vectors where each value represents, for example, the number of occurrences of a term or its relative frequency. If the feature vectors have n elements and each of them can assume k different values with probability pk, then:\n",
    "\n",
    "Bernoulli naive Bayes\n",
    "If X is random variable Bernoulli-distributed, it can assume only two values (for simplicity, let’s call them 0 and 1) and their probability is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f20579-f1f7-4405-9aed-4d7ec4330ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    " When should you use Gaussian Naïve Bayes over other variants\n",
    "Gaussian Naive Bayes is a classification algorithm that extends the Naive Bayes algorithm to handle continuous data by assuming that the data follows a Gaussian (normal) distribution. This algorithm is particularly useful for tasks where the features are continuous and normally distributed.\n",
    "\n",
    "Key Principles\n",
    "\n",
    "Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes Classifier is based on Bayes' Theorem, which provides a way to update the probability of a hypothesis based on new evidence. The theorem is given by:\n",
    "\n",
    "[ P(y|x) = \\frac{P(x|y) \\cdot P(y)}{P(x)} ]\n",
    "\n",
    "Where:\n",
    "\n",
    "( P(y|x) ) is the posterior probability of class ( y ) given feature ( x ).\n",
    "\n",
    "( P(x|y) ) is the likelihood of feature ( x ) given class ( y ).\n",
    "\n",
    "( P(y) ) is the prior probability of class ( y ).\n",
    "\n",
    "( P(x) ) is the probability of feature ( x )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82baf5-604b-4dd6-9398-0f96eb16dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are the key assumptions made by Naïve Bayes\n",
    "It is based on Bayes’ theorem and assumes that features are conditionally independent of each other given the class label. The algorithm calculates the probability of a data point belonging to each class and assigns it to the class with the highest probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b589f-901d-4e29-851c-cc8ce49ad742",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are the advantages and disadvantages of Naïve Bayes\n",
    "Simple to implement:Naive Bayes classifier is a very simple algorithm and easy to implement. It does not require a lot of computation or training time. It can be used for both binary and multiple class classification related tasks.\n",
    "\n",
    "2.Handles missing data well:This algorithm is very useful for handling missing data as well. For accuracy measure, this classifier considers only present data and neglect the data which is not present. By this the accuracy is maintained.\n",
    "\n",
    "3.Fast and scalable:Naive Bayes classifier is fast and scalable in nature and can work with large datasets.It can be used for fast learning and real-time classification tasks, and it can be easily parallelized to run on multiple processors or clusters.\n",
    "\n",
    "4.Simple to understand:Naive Bayes is simple to understand because it gives a detailed explanation of how the classification is carried out. Based on the presence or absence of each feature, it determines the probability of a specific result and assigns a class based on the highest probability.\n",
    "\n",
    "5.Performs well in text classification:Naive Bayes is a well-liked algorithm for text classification tasks, like sentiment analysis or spam filtering. It performs well. This is due to the fact that it is capable of handling high-dimensional data and performs well with categorical data, both of which are common in natural language processing.\n",
    "\n",
    "Assumption of independence:The algorithm makes the assumption that all features are independent of one another, which is frequently false in practical applications. If the features are correlated, this may result in inaccurate classification results.\n",
    "\n",
    "2.Lack of flexibility:Because Naive Bayes is a parametric model, it needs a set of predetermined parameters that must be learned from training data. Its ability to handle complicated and non-linear relationships between features may be constrained as a result.\n",
    "\n",
    "3.Data scarcity:For Naive Bayes to accurately estimate the conditional probabilities of each feature, there must be enough training data. Insufficient training data may cause the algorithm to underperform.\n",
    "\n",
    "4.Sensitivity to outliers:Naive Bayes is sensitive to outliers or extreme values in the data, which can have a significant impact on the estimated probabilities and produce incorrect classification outcomes.\n",
    "\n",
    "5.Class imbalance:When data are unbalanced and one class has significantly more samples than the other, naive Bayes can have trouble handling the situation. This may result in bias in favour of the majority class and suboptimal performance on the part of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f88a1-6d32-4931-8b61-d240762500f4",
   "metadata": {},
   "outputs": [],
   "source": [
    " Why is Naïve Bayes a good choice for text classification\n",
    "It is used mostly in high-dimensional text classification The Naive Bayes Classifier is a simple probabilistic classifier and it has very few number of parameters which are used to build the ML models that can predict at a faster speed than other classification algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74dae5d-bd3c-47cf-bd44-f6d6a8c83143",
   "metadata": {},
   "outputs": [],
   "source": [
    " Compare SVM and Naïve Bayes for classification tasks\n",
    " In this tutorial, we’ll be analyzing the methods Naïve Bayes (NB) and Support Vector Machine (SVM). We contrast the advantages and disadvantages of those methods for text classification. We’ll compare them from theoretical and practical perspectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1efbe-2968-4cc4-b00f-e6d7079110d9",
   "metadata": {},
   "outputs": [],
   "source": [
    " How does Laplace Smoothing help in Naïve Bayes?\n",
    "Laplace smoothing is a technique used to handle the problem of zero probability in the Naive Bayes algorithm. Naive Bayes is a probabilistic classifier based on Bayes' theorem and is commonly used for classification tasks such as spam filtering and sentiment analysis\n",
    "1\n",
    "2\n",
    ".\n",
    "\n",
    "The Zero Probability Problem\n",
    "\n",
    "In Naive Bayes, we calculate the probability of a class given a set of features. However, if a feature in the test data is not present in the training data, its probability is zero. This leads to the entire probability of the class being zero, which is problematic\n",
    "1\n",
    "2\n",
    ".\n",
    "\n",
    "For example, consider a text classification task where we want to classify a review as positive or negative. If a word in the review is not present in the training data, its probability is zero, making the overall probability of the review being positive or negative zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59226e8-81a9-4bb4-9fb5-960bc200ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the SVM classifier on test data: {acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f83c9-c873-4812-a3b6-7618b354018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then \n",
    "compare their accuracies\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize SVM classifiers with different kernels\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Train the classifiers\n",
    "svm_linear.fit(X_train, y_train)\n",
    "svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f03e0-e60a-44f2-a05d-3905527daca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean \n",
    "Squared Error (MSE)\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVR model with RBF kernel (default)\n",
    "svr = SVR(kernel='rbf')\n",
    "\n",
    "# Train the SVR model\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b816ce-8dfc-4e30-a39e-15ca08432698",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision \n",
    "boundary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate 2D data with 2 classes\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=42, cluster_std=1.5)\n",
    "\n",
    "# Train SVM classifier with Polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3, coef0=1, C=1)\n",
    "svm_poly.fit(X, y)\n",
    "\n",
    "# Create a mesh grid for plotting decisio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c919763-49d7-4fb9-aeed-52bea53e9326",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and \n",
    "evaluate accuracy\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Gaussian Naïve Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Gaussian Naïve Bayes classifier: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45f27d-67db-46eb-8005-493f35b92493",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 \n",
    "Newsgroups dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load 20 Newsgroups dataset (subset for faster training, e.g., categories='all')\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "# Split into train and test sets (75% train, 25% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a pipeline that vectorizes the text data then applies Multinomial Naive Bayes\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the classifier\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Multinomial Naïve Bayes classifier on 20 Newsgroups: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71d1f7-32d6-4fae-a990-d063a910f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train an SVM Classifier with different C values and compare the decision \n",
    "boundaries visually\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate 2D dataset with 2 classes\n",
    "X, y = make_blobs(n_samples=100, centers=2, cluster_std=1.2, random_state=42)\n",
    "\n",
    "# Different values of C to compare\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Set up mesh grid for plotting decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n",
    "                     np.linspace(y_min, y_max, 400))\n",
    "\n",
    "plt.figure(figsize=(18, 3.5))\n",
    "\n",
    "for i, C in enumerate(C_values, 1):\n",
    "    # Train SVM with RBF kernel and current C value\n",
    "    svm = SVC(kernel='rbf', C=C, gamma='scale')\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    # Predict on the mesh grid\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.subplot(1, len(C_values), i)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.6, cmap=plt.cm.coolwarm)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, edgecolors='k', cmap=plt.cm.coolwarm)\n",
    "    plt.title(f\"SVM with C={C}\")\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8fe086-56a1-4bfe-a849-75b9c48531c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with \n",
    "binary feature\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a simple synthetic dataset with binary features\n",
    "# 0/1 features, 100 samples, 5 features each\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(2, size=(100, 5))  # Binary features\n",
    "y = np.random.randint(2, size=100)       # Binary labels (0 or 1)\n",
    "\n",
    "# Split dataset into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Bernoulli Naïve Bayes classifier\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Train the classifier\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Bernoulli Naïve Bayes classifier: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb1da7-4520-4e2d-86a8-e006a33d3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to apply feature scaling before training an SVM model and compare results with \n",
    "unscaled data\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --------- Without feature scaling ---------\n",
    "svm_unscaled = SVC(kernel='rbf', random_state=42)\n",
    "svm_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "\n",
    "# --------- With feature scaling ---------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svm_scaled = SVC(kernel='rbf', random_state=42)\n",
    "svm_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213dde2e-4b69-47f0-8958-f7f04a09ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, \n",
    "gamma, kernel)\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],           # Regularization parameter\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Kernel coefficient\n",
    "    'kernel': ['linear', 'rbf', 'poly']         # Kernel types\n",
    "}\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Train models and find best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on test data using the best estimator\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with best parameters: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8332d24-2b31-43f4-8801-f612469cc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and \n",
    "check it improve accuracy\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_classes=2,\n",
    "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1 (imbalanced)\n",
    "    flip_y=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM without class weig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bc16e-97ae-4528-b3d9-ee52e013ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to implement a Naïve Bayes classifier for spam detection using email data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Download dataset from URL\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "# Map labels to binary values: ham=0 (not spam), spam=1\n",
    "df['label_num'] = df.label.map({'ham': 0, 'spam': 1})_]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6be76-1ea0-4b8f-89c1-85cca7be1468",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and \n",
    "compare their accuracy\n",
    "     from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d2394-4ec4-471b-bab3-55454a36fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare \n",
    "results\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# -------- Without feature selection --------\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "accuracy_without_fs = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# -------- With feature selection --------\n",
    "# Select top k features (e.g., k=10) using chi-squared test\n",
    "selector = SelectKBest(score_func=chi2, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "gnb_fs = GaussianNB()\n",
    "gnb_fs.fit(X_train_selected, y_train)\n",
    "y_pred_fs = gnb_fs.predict(X_test_selected)\n",
    "accuracy_with_fs = accuracy_score(y_test, y_pred_fs)\n",
    "\n",
    "print(f\"Accuracy without feature selection: {accuracy_without_fs:.4f}\")\n",
    "print(f\"Accuracy with feature selection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c22c65-f5dd-45de-9100-317256f84101",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) \n",
    "strategies on the Wine dataset and compare their accuracy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split into train/test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Base SVM classifier\n",
    "base_svm = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
    "\n",
    "# One-vs-Rest strategy\n",
    "ovr = OneVsRestClassifier(base_svm)\n",
    "ovr.fit(X_train, y_train)\n",
    "y_pred_ovr = ovr.predict(X_test)\n",
    "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
    "\n",
    "# One-vs-One strategy\n",
    "ovo = OneVsOneClassifier(base_svm)\n",
    "ovo.fit(X_train, y_train)\n",
    "y_pred_ovo = ovo.predict(X_test)\n",
    "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
    "\n",
    "print(f\"Accuracy with One-vs-Rest (OvR): {accuracy_ovr:.4f}\")\n",
    "print(f\"Accuracy with One-vs-One (OvO): {accuracy_ovo:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e30f9-8dcc-46b2-bf2a-dea211889d85",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast \n",
    "Cancer dataset and compare their accuracy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Breast Cancer dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into train/test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define kernels to test\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "for kernel in kernels:\n",
    "    # Initialize SVM with the kernel\n",
    "    svm = SVC(kernel=kernel, gamma='scale', random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = svm.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy with {kernel} kernel: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f2edd-dba8-4fcb-b429-f76a5841c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the \n",
    "average accuracy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Initialize Stratified K-Fold with 5 splits\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Initialize and train SVM classifier\n",
    "    svm = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb620d63-67f6-4438-bdb8-e46cbbaed356",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare \n",
    "performance\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Different prior probabilities to test\n",
    "priors_list = [\n",
    "    None,                # Use default (estimated from data)\n",
    "    [0.5, 0.5],          #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46459f-6fb9-4df3-9e42-b9306d0311ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and \n",
    "compare accuracy\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --------- Without feature selection ---------\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_p_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4be283-7e2d-464c-b4f4-27188217ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and \n",
    "F1-Score instead of accuracy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM classifier\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Alternatively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954e7f62-c909-4d53-b0fa-5d0675a719ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss \n",
    "(Cross-Entropy Loss)\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for test data\n",
    "y_prob = gnb.predict_proba(X_test)\n",
    "\n",
    "# Calculate Log Loss\n",
    "loss = log_loss(y_test, y_prob)\n",
    "\n",
    "print(f\"Log Loss (Cross-Entropy Loss): {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00292be6-7c65-41dc-930e-d6a5639092b3",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac683dcd-fef2-44a7-81a6-c421c0e445fa",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute \n",
    "Error (MAE) instead of MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851534f4-1e61-4a1c-be89-b35e66b41f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3feb0-c3ab-4161-9907-113adbd62465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2c99e-6230-4a95-8da1-302b7fd27dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb07a8b-74ee-4ecd-9ceb-c248c4945bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdccc64-fe0f-4b09-bc17-d26362ea78e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
